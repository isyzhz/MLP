{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module:\n",
    "    def __init__(self):\n",
    "        self.with_weights = False\n",
    "        pass\n",
    "    def forward(self):\n",
    "        pass\n",
    "    def backward(self):\n",
    "        pass\n",
    "    def __call__(self, *inputs, **kwargs):\n",
    "        return self.forward(*inputs, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Linear, self).__init__()\n",
    "        # Initialize weights\n",
    "        self.W = np.random.randn(input_dim, output_dim) * 1e-2\n",
    "        self.b = np.zeros((1, output_dim))\n",
    "        # The linear layer's parameters should be updated\n",
    "        self.with_weights = True\n",
    "        \n",
    "    def forward(self, input_array):\n",
    "        res = np.zeros((input_array.shape[0], self.W.shape[1]))\n",
    "        res = np.dot(input_array, self.W) + self.b\n",
    "\n",
    "        return res\n",
    "\n",
    "    def backward(self, input_array, output_gradient, lr=0.05):\n",
    "        res = np.zeros_like(input_array)\n",
    "        \n",
    "        # 1. Compute the gradient to be passed to the previous layer (dL/dX)\n",
    "        res = np.dot(output_gradient, self.W.T)\n",
    "        \n",
    "        # 2. Compute the gradients for W and b, then update W and b\n",
    "        dW = np.dot(input_array.T, output_gradient)\n",
    "        db = np.sum(output_gradient, axis=0, keepdims=True)\n",
    "        \n",
    "        # Update the weights and biases\n",
    "        self.W -= lr * dW\n",
    "        self.b -= lr * db\n",
    "        \n",
    "        return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    def __init__(self):\n",
    "        super(ReLU, self).__init__()\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input_array):\n",
    "        res = np.zeros_like(input_array)        \n",
    "        res = np.maximum(0, input_array)       \n",
    "        return res\n",
    "\n",
    "    def backward(self, input_array, output_gradient):\n",
    "        res = np.zeros_like(input_array)        \n",
    "        res = output_gradient * (input_array > 0).astype(float)       \n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Module):\n",
    "    def __init__(self):\n",
    "        super(Sigmoid, self).__init__()\n",
    "\n",
    "    def forward(self, input_array):\n",
    "        return 1 / (1 + np.exp(-input_array))\n",
    "\n",
    "    def backward(self, input_array, output_gradient):\n",
    "        return output_gradient * self.forward(input_array) * (1 - self.forward(input_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss(Module):\n",
    "    def __init__(self):\n",
    "        super(MSELoss, self).__init__()   \n",
    "        \n",
    "    def forward(self, predicted_y, y):\n",
    "        return np.mean((predicted_y - y) ** 2)\n",
    "\n",
    "    def backward(self, predicted_y, y):\n",
    "        return (predicted_y - y) * 2 / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(Module):\n",
    "    def __init__(self, layers, loss, lr = 0.05):\n",
    "        super(SimpleNN, self).__init__()    \n",
    "        self.layers = layers\n",
    "        self.loss = loss\n",
    "        self.inputs = [None for _ in range(len(self.layers))]\n",
    "        self.output = None\n",
    "        self.loss_value = 1e5\n",
    "        self.lr = lr\n",
    "        pass\n",
    "\n",
    "    def forward(self, input_array):\n",
    "        current_input = input_array\n",
    "        for i in range(len(self.layers)):\n",
    "            self.inputs[i] = current_input\n",
    "            current_input = self.layers[i](current_input)\n",
    "        self.output = current_input\n",
    "        return self.output\n",
    "        \n",
    "    def backward(self, y):\n",
    "        if self.inputs[-1] is None:\n",
    "            print(\"call forward first.\")\n",
    "            return\n",
    "        self.loss_value = self.loss(self.output, y)\n",
    "        output_gradient = self.loss.backward(self.output, y)\n",
    "        for i in range(len(self.layers)-1, -1, -1):\n",
    "            # here we check wheather a layer should be updated in the backpropagation by judging its `.with_weights` value.\n",
    "            # whether we update the weight is determined by whether the layer has trainable parameters. \n",
    "            # for instance, a layer such as x^2 or ||x||^2 does not have a parameter, so we set `.with_weight`` to false.\n",
    "            if not isinstance(self.inputs[i], np.ndarray):\n",
    "                self.inputs[i]=self.inputs[i].numpy()\n",
    "            if self.layers[i].with_weights:\n",
    "                output_gradient = self.layers[i].backward(self.inputs[i], output_gradient, self.lr)\n",
    "            else:\n",
    "                output_gradient = self.layers[i].backward(self.inputs[i], output_gradient)\n",
    "        self.output = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [\n",
    "    Linear(2, 20),\n",
    "    Sigmoid(),\n",
    "    Linear(20, 2)\n",
    "]\n",
    "\n",
    "loss = MSELoss()\n",
    "model = SimpleNN(layers, loss, lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0/50000, loss: 16.598175\n",
      "epoch 500/50000, loss: 0.089535\n",
      "epoch 1000/50000, loss: 0.041829\n",
      "epoch 1500/50000, loss: 0.033773\n",
      "epoch 2000/50000, loss: 0.032144\n",
      "epoch 2500/50000, loss: 0.030796\n",
      "epoch 3000/50000, loss: 0.029570\n",
      "epoch 3500/50000, loss: 0.028425\n",
      "epoch 4000/50000, loss: 0.027342\n",
      "epoch 4500/50000, loss: 0.026306\n",
      "epoch 5000/50000, loss: 0.025309\n",
      "epoch 5500/50000, loss: 0.024346\n",
      "epoch 6000/50000, loss: 0.023411\n",
      "epoch 6500/50000, loss: 0.022737\n",
      "epoch 7000/50000, loss: 0.024539\n",
      "epoch 7500/50000, loss: 0.024311\n",
      "epoch 8000/50000, loss: 0.024040\n",
      "epoch 8500/50000, loss: 0.023742\n",
      "epoch 9000/50000, loss: 0.023430\n",
      "epoch 9500/50000, loss: 0.023111\n",
      "epoch 10000/50000, loss: 0.022792\n",
      "epoch 10500/50000, loss: 0.022476\n",
      "epoch 11000/50000, loss: 0.022167\n",
      "epoch 11500/50000, loss: 0.021866\n",
      "epoch 12000/50000, loss: 0.021573\n",
      "epoch 12500/50000, loss: 0.021291\n",
      "epoch 13000/50000, loss: 0.021017\n",
      "epoch 13500/50000, loss: 0.020754\n",
      "epoch 14000/50000, loss: 0.020499\n",
      "epoch 14500/50000, loss: 0.020253\n",
      "epoch 15000/50000, loss: 0.020015\n",
      "epoch 15500/50000, loss: 0.019785\n",
      "epoch 16000/50000, loss: 0.019562\n",
      "epoch 16500/50000, loss: 0.019345\n",
      "epoch 17000/50000, loss: 0.019135\n",
      "epoch 17500/50000, loss: 0.018930\n",
      "epoch 18000/50000, loss: 0.018730\n",
      "epoch 18500/50000, loss: 0.018535\n",
      "epoch 19000/50000, loss: 0.018345\n",
      "epoch 19500/50000, loss: 0.018159\n",
      "epoch 20000/50000, loss: 0.017976\n",
      "epoch 20500/50000, loss: 0.017797\n",
      "epoch 21000/50000, loss: 0.017622\n",
      "epoch 21500/50000, loss: 0.017450\n",
      "epoch 22000/50000, loss: 0.017281\n",
      "epoch 22500/50000, loss: 0.017114\n",
      "epoch 23000/50000, loss: 0.016951\n",
      "epoch 23500/50000, loss: 0.016790\n",
      "epoch 24000/50000, loss: 0.016632\n",
      "epoch 24500/50000, loss: 0.016476\n",
      "epoch 25000/50000, loss: 0.016323\n",
      "epoch 25500/50000, loss: 0.016172\n",
      "epoch 26000/50000, loss: 0.016023\n",
      "epoch 26500/50000, loss: 0.015876\n",
      "epoch 27000/50000, loss: 0.015732\n",
      "epoch 27500/50000, loss: 0.015589\n",
      "epoch 28000/50000, loss: 0.015449\n",
      "epoch 28500/50000, loss: 0.015311\n",
      "epoch 29000/50000, loss: 0.015174\n",
      "epoch 29500/50000, loss: 0.015040\n",
      "epoch 30000/50000, loss: 0.014907\n",
      "epoch 30500/50000, loss: 0.014777\n",
      "epoch 31000/50000, loss: 0.014648\n",
      "epoch 31500/50000, loss: 0.014521\n",
      "epoch 32000/50000, loss: 0.014396\n",
      "epoch 32500/50000, loss: 0.014272\n",
      "epoch 33000/50000, loss: 0.014150\n",
      "epoch 33500/50000, loss: 0.014030\n",
      "epoch 34000/50000, loss: 0.013912\n",
      "epoch 34500/50000, loss: 0.013795\n",
      "epoch 35000/50000, loss: 0.013680\n",
      "epoch 35500/50000, loss: 0.013567\n",
      "epoch 36000/50000, loss: 0.013455\n",
      "epoch 36500/50000, loss: 0.013345\n",
      "epoch 37000/50000, loss: 0.013237\n",
      "epoch 37500/50000, loss: 0.013129\n",
      "epoch 38000/50000, loss: 0.013024\n",
      "epoch 38500/50000, loss: 0.012920\n",
      "epoch 39000/50000, loss: 0.012817\n",
      "epoch 39500/50000, loss: 0.012716\n",
      "epoch 40000/50000, loss: 0.012617\n",
      "epoch 40500/50000, loss: 0.012518\n",
      "epoch 41000/50000, loss: 0.012422\n",
      "epoch 41500/50000, loss: 0.012326\n",
      "epoch 42000/50000, loss: 0.012232\n",
      "epoch 42500/50000, loss: 0.012140\n",
      "epoch 43000/50000, loss: 0.012049\n",
      "epoch 43500/50000, loss: 0.011960\n",
      "epoch 44000/50000, loss: 0.009922\n",
      "epoch 44500/50000, loss: 0.013142\n",
      "epoch 45000/50000, loss: 0.012079\n",
      "epoch 45500/50000, loss: 0.011804\n",
      "epoch 46000/50000, loss: 0.010572\n",
      "epoch 46500/50000, loss: 0.012180\n",
      "epoch 47000/50000, loss: 0.011722\n",
      "epoch 47500/50000, loss: 0.011415\n",
      "epoch 48000/50000, loss: 0.010475\n",
      "epoch 48500/50000, loss: 0.011760\n",
      "epoch 49000/50000, loss: 0.011387\n",
      "epoch 49500/50000, loss: 0.010816\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "train_data = pd.read_csv('train_data.csv')\n",
    "\n",
    "x1 = (train_data['H1'] + train_data['H2'] + train_data['H3']) / 3\n",
    "x2 = train_data['Pdeficit']\n",
    "\n",
    "y1 = train_data['f_drop_real']\n",
    "y2 = train_data['t_nadir_real']\n",
    "\n",
    "# Combine data into X and Y for training\n",
    "X_train = np.column_stack((x1, x2))\n",
    "Y_train = np.column_stack((y1, y2))\n",
    "\n",
    "epoch = 0\n",
    "max_iteration = 50000\n",
    "loss_values = []  \n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "pred_y = model(X_train)\n",
    "\n",
    "convergence_threshold = 1e-4\n",
    "convergence_epoch = None  \n",
    "\n",
    "\n",
    "while epoch < max_iteration and model.loss_value > convergence_threshold:\n",
    "    pred_y = model(X_train)\n",
    "    model.backward(Y_train)\n",
    "    loss_values.append(model.loss_value)  \n",
    "    if epoch % 500 == 0:\n",
    "        print(f\"epoch {epoch}/{max_iteration}, loss: {model.loss_value:.6f}\")\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for y1 (f_drop_real): 0.039366\n",
      "MAE for y2 (t_nadir_real): 0.067993\n"
     ]
    }
   ],
   "source": [
    "test_data = pd.read_csv('test_data.csv')\n",
    "\n",
    "x1_test = (test_data['H1'] + test_data['H2'] + test_data['H3']) / 3\n",
    "x2_test = test_data['Pdeficit']\n",
    "\n",
    "y1_test = test_data['f_drop_real']\n",
    "y2_test = test_data['t_nadir_real']\n",
    "\n",
    "X_test = np.column_stack((x1_test, x2_test))\n",
    "Y_test = np.column_stack((y1_test, y2_test))\n",
    "\n",
    "pred_test = model(X_test)\n",
    "\n",
    "mae_y1 = mean_absolute_error(Y_test[:, 0], pred_test[:, 0])\n",
    "mae_y2 = mean_absolute_error(Y_test[:, 1], pred_test[:, 1])\n",
    "\n",
    "print(f\"MAE for y1 (f_drop_real): {mae_y1:.6f}\")\n",
    "print(f\"MAE for y2 (t_nadir_real): {mae_y2:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
